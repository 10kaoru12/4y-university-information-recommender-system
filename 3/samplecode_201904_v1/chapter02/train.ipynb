{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "target_authors = [\n",
    "    \"太宰治\", \"海野十三\", \"芥川龍之介\", \"宮沢賢治\",\n",
    "    \"岡本かの子\", \"江戸川乱歩\", \"夏目漱石\", \"紫式部\",\n",
    "]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(target_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "X, y = [], []\n",
    "with open(\"stories.jsonl\", \"r\") as stories:\n",
    "    for raw_story in stories:\n",
    "        story = json.loads(raw_story)\n",
    "        if story[\"author\"] in target_authors:\n",
    "            X.append(story[\"body\"])\n",
    "            y.append(story[\"author\"])\n",
    "y = le.transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "story_clf = Pipeline([\n",
    "    (\"tokenize\", MeCabTokenize(pos_keep_filters=[\"名詞\", \"形容詞\", \"動詞\"])),\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class Token:\n",
    "    \"\"\"MeCabのトークンを保持するクラス\"\"\"\n",
    "    def __init__(self, node):\n",
    "        # 表層形\n",
    "        self.surface = node.surface\n",
    "\n",
    "        features = node.feature.split(\",\")\n",
    "        # 品詞\n",
    "        self.part_of_speech = features[0]\n",
    "        # 基本形\n",
    "        self.base_form = features[6]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{}\\t{}\".format(self.surface, self.part_of_speech)\n",
    "\n",
    "\n",
    "class MeCabTokenize(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"MeCabを利用して形態素解析を行うクラス\"\"\"\n",
    "    def __init__(self, pos_keep_filters=[]):\n",
    "        # MeCabインスタンスの生成\n",
    "        self.tokenizer = MeCab.Tagger(\"-b 100000\")\n",
    "        # メモリの初期化周りでバグがあるため、一度解析することで回避\n",
    "        self.tokenizer.parse(\"init\")\n",
    "\n",
    "        # 前処理を手軽に行えるように、品詞フィルタを作る\n",
    "        self.pos_keep_filters = pos_keep_filters\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # scikit-learn互換のインターフェイス\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # scikit-learn互換のインターフェイス\n",
    "        docs = []\n",
    "        # 1文書ずつ処理する\n",
    "        for text in X:\n",
    "            words = []\n",
    "            # 文書内のテキストを改行でさらに文に分ける\n",
    "            for sentence in self.split_text_to_sentences(text):\n",
    "                # 対象の文を分かち書き\n",
    "                words.extend(self.wakati(sentence))\n",
    "            # 文書はスペース区切りで追加する\n",
    "            docs.append(\" \".join(words))\n",
    "        return docs\n",
    "\n",
    "    def split_text_to_sentences(self, text, delimiter=\"。\"):\n",
    "        return [t for t in text.replace(delimiter, delimiter + \"\\n\").splitlines() if t]\n",
    "\n",
    "    def wakati(self, text):\n",
    "        return [t.base_form for t in self.tokenize(text)]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "\n",
    "        node = self.tokenizer.parseToNode(text)\n",
    "        node = node.next\n",
    "        while node.next:\n",
    "            token = Token(node)\n",
    "            # 品詞フィルターを適用する\n",
    "            if token.part_of_speech in self.pos_keep_filters:\n",
    "                tokens.append(Token(node))\n",
    "            node = node.next\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                      X,  # 入力\n",
    "                                      y, # 正解ラベル\n",
    "                                      test_size=0.3, # テストデータのサイズ（全体の何割か）\n",
    "                                      random_state=42, # シャッフルしたときの乱数の種を固定\n",
    "                                      shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "story_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = story_clf.predict(X_test)\n",
    "# 出力は数値なので、ラベルへと逆変換するために、LabelEncoderのclasses_を入力する\n",
    "print(classification_report(y_test, pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習データのテキストごとに文字集合を作成\n",
    "train_set = [set(train_x) for train_x in X_train]\n",
    "\n",
    "for test_idx, test in tqdm(enumerate(X_test)):\n",
    "    test = set(test)\n",
    "    for train_idx, train in enumerate(train_set):\n",
    "\t    # 学習データとテストデータで同じ文字集合か確認\n",
    "        if test == train:\n",
    "            print(test_idx, train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pandasにより重複を排除したindexを取得\n",
    "X_set = [set(x) for x in X]\n",
    "df = pd.DataFrame(X_set)\n",
    "df_index = df.drop_duplicates().index\n",
    "target_indices = set(df_index.tolist())\n",
    "\n",
    "# 重複を排除した学習用データを取得\n",
    "new_X = [nx for i, nx in enumerate(X) if i in target_indices]\n",
    "new_y = [ny for i, ny in enumerate(y) if i in target_indices]\n",
    "\n",
    "# サイズの確認\n",
    "print(len(new_X), len(X))\n",
    "print(len(new_y), len(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                      new_X,  # 入力\n",
    "                                      new_y, # 正解ラベル\n",
    "                                      test_size=0.3, # テストデータのサイズ（全体の何割か）\n",
    "                                      random_state=42, # シャッフルしたときの乱数の種を固定\n",
    "                                      shuffle=True\n",
    ")\n",
    "\n",
    "story_clf.fit(X_train, y_train)\n",
    "\n",
    "pred = story_clf.predict(X_test)\n",
    "# 出力は数値なので、ラベルへと逆変換するために、LabelEncoderのclasses_を入力する\n",
    "print(classification_report(y_test, pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "for i in np.where(le.inverse_transform(new_y) == \"宮沢賢治\")[0]:\n",
    "    print(new_X[i][:100])\n",
    "    print(\"...\")\n",
    "    print(new_X[i][-100:])\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, pred, labels=le.transform(le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# フォントキャッシュの削除\n",
    "matplotlib.font_manager._rebuild()\n",
    "\n",
    "# 日本語フォントの設定\n",
    "matplotlib.rcParams['font.family'] = \"IPAGothic\"\n",
    "\n",
    "cm = confusion_matrix(y_test, pred, labels=le.transform(le.classes_))\n",
    "\n",
    "# 値の正規化\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# 混同行列を画像として描画する。カラーマップは青系統を利用\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# タイトルを追加\n",
    "plt.title(\"著者分類\")\n",
    "# カラーバーを追加\n",
    "plt.colorbar()\n",
    "\n",
    "# x軸とy軸のそれぞれの要素のラベルを追加\n",
    "tick_marks = np.arange(len(le.classes_))\n",
    "plt.xticks(tick_marks, le.classes_, rotation=90)\n",
    "plt.yticks(tick_marks, le.classes_)\n",
    "\n",
    "# 各要素に数値を描画する。閾値は色の変更のために利用する\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], \".2f\"),\n",
    "                  horizontalalignment=\"center\",\n",
    "                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# 軸ラベルの追加\n",
    "plt.ylabel('正解ラベル')\n",
    "plt.xlabel('推定ラベル')\n",
    "\n",
    "# 保存したい場合\n",
    "# plt.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
